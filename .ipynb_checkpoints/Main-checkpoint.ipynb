{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "acda35d9",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7149dcc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import energym\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import sklearn as scipy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import warnings\n",
    "import plotly.graph_objects as go\n",
    "import optuna\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import Matern\n",
    "from scipy.optimize import Bounds\n",
    "from scipy.optimize import minimize\n",
    "from tqdm import tqdm\n",
    "from plotly.subplots import make_subplots\n",
    "from sklearn.model_selection import train_test_split\n",
    "from itertools import product\n",
    "from skopt import Optimizer\n",
    "from skopt import gp_minimize\n",
    "from skopt.space import Real\n",
    "from optuna.samplers import TPESampler\n",
    "from optuna.samplers import MOTPESampler\n",
    "\n",
    "#Functions:\n",
    "%run Functions.ipynb\n",
    "\n",
    "# Setup Parameters:\n",
    "CelsiusToKelvin = 273.15\n",
    "RunMPC = 1 # True = runs mpc, False = load mpc data\n",
    "simNewData = 1\n",
    "trainNewModel = 1\n",
    "ChosenPredModel = 1  #0 for PINN, 1 for nRnC\n",
    "downsampleNumber = 3 # !! IF YOU CHANGE THIS, YOU HAVE TO simNewData and trainNewModel !!. Given in multipla of 5 minutes\n",
    "chunkTrainingPercentage = 0.01       #This is tricky to explain. We determine at how many points out of the total number of points in the training data we start the chunk training from.\n",
    "Plot_Modelperformance = 0 \n",
    "trainPredHours = 3\n",
    "predTestArray = np.arange(1, trainPredHours+1)\n",
    "prevOutputRArray = np.ones(4)\n",
    "nRnCOutputSamples = 64000\n",
    "hyperParamTuner = 0 #0 for single objective, 1 for multi\n",
    "\n",
    "# Simulation Parameters:\n",
    "TimeStamps = ['08:00', '15:00']  # Selected times for change of temperature reference\n",
    "Temp_Ref = [21, 17]              # Chosen temperature references\n",
    "T_ref_Deviation = 1              # Temperature deviation in dregrees celcius\n",
    "NumberOfBuildings = 3            # number of building chosen to run in simulation\n",
    "simulationDays = round(365)      # Days to simulate for training the model\n",
    "P = 0.3                           # P control gain\n",
    "\n",
    "T_RefVec = [19,19,20,20,22,20,21,24,22,23,21,22,21,21,20] # Temperature references\n",
    "T_Day_Night_Change = [1, 1, 2, 1, 2, 1, 1, 2, 1, 1, 1, 2, 2, 1, 1] # change day and night\n",
    "Data_Temp_Ref = [T_RefVec, T_Day_Night_Change]\n",
    "\n",
    "numberOfWeightInitializations = 10\n",
    "\n",
    "#Time setup\n",
    "# timeInterval = 10                        # given in minutes\n",
    "timeInterval = 5 * downsampleNumber\n",
    "dt = 1/(60*24) * timeInterval\n",
    "steps_per_Day = round(1/dt)\n",
    "T_RefVec = np.array(T_RefVec)\n",
    "T_RefVec = T_RefVec + CelsiusToKelvin\n",
    "#T_Ref = T_Ref + CelsiusToKelvin\n",
    "\n",
    "%run Functions.ipynb\n",
    "# MPC Parameters:\n",
    "final_sim_days = 185    # Number of MPC simulation days\n",
    "H_p = 8                   # Prediction Horizon\n",
    "H_c = H_p                  # Control Horizon\n",
    "R = 500                    # Punishment for Temperature Error\n",
    "CFirstDay = 400\n",
    "C_deltaFirstDay = 400\n",
    "\n",
    "i_off = 0                  # Is used for offsetting simulation\n",
    "steps_per_MPCout = 1       # Steps the MPC does per iteration\n",
    "Input_Max_Value = 25000    # Maximum input value\n",
    "\n",
    "#Misc MPC/MOBO setup:\n",
    "u_max = Bounds(0, Input_Max_Value)\n",
    "u0 = np.ones(H_p)*100\n",
    "MPC_Iter= steps_per_Day * final_sim_days * downsampleNumber\n",
    "\n",
    "#KPI inits\n",
    "Ref_change_penalty_duration = 120 #given in minutes\n",
    "Ref_change_penalty_length = (Ref_change_penalty_duration)/5\n",
    "paramTuningInterval = 3 #given in days."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28535d64",
   "metadata": {},
   "source": [
    "# Create Temperature Reference Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0dc71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_time1 = pd.to_datetime(TimeStamps[0], format='%H:%M')\n",
    "selected_time2 = pd.to_datetime(TimeStamps[1], format='%H:%M')\n",
    "\n",
    "Timestamp1 = round((selected_time1.hour * 60 + selected_time1.minute) / 5)\n",
    "Timestamp2 = round((selected_time2.hour * 60 + selected_time2.minute) / 5)\n",
    "\n",
    "Time_stamps = [Timestamp1, Timestamp2]\n",
    "Temp_Ref = np.array(Temp_Ref)\n",
    "Temp_Ref = Temp_Ref + CelsiusToKelvin\n",
    "\n",
    "# Ref Temp for simulation\n",
    "T_ref_Vector = np.zeros(steps_per_Day*downsampleNumber)\n",
    "for k in range(steps_per_Day*downsampleNumber):\n",
    "    if k < Timestamp1:\n",
    "        T_ref_Vector[k] = Temp_Ref[-1] \n",
    "    elif k >= Timestamp1 and k <= Timestamp2:\n",
    "        T_ref_Vector[k] = Temp_Ref[0] \n",
    "    elif k > Timestamp2:\n",
    "        T_ref_Vector[k] = Temp_Ref[-1]\n",
    "        \n",
    "T_ref_Vector = T_ref_Vector \n",
    "T_ref_Lower = T_ref_Vector - T_ref_Deviation\n",
    "T_ref_Upper = T_ref_Vector + T_ref_Deviation\n",
    "\n",
    "Data_Temp_Ref[0]\n",
    "Day_Mean = Data_Temp_Ref[0]\n",
    "Day_Night = Data_Temp_Ref[-1]\n",
    "\n",
    "# Ref temp for data generation\n",
    "T_ref_Vector_DataGen = np.zeros(steps_per_Day*downsampleNumber)\n",
    "tester = np.zeros(steps_per_Day*downsampleNumber * len(Day_Mean))\n",
    "for i in range(len(Day_Mean)):\n",
    "    for k in range(steps_per_Day*downsampleNumber):\n",
    "        if k < Timestamp1:\n",
    "            T_ref_Vector_DataGen[k] = Day_Mean[i] - Day_Night[i]\n",
    "        elif k >= Timestamp1 and k <= Timestamp2:\n",
    "            T_ref_Vector_DataGen[k] = Day_Mean[i] + Day_Night[i] \n",
    "        elif k > Timestamp2:\n",
    "            T_ref_Vector_DataGen[k] = Day_Mean[i] - Day_Night[i]\n",
    "    if i == 0:\n",
    "        Data_Generation_Ref = T_ref_Vector_DataGen.copy()\n",
    "    elif i > 0:\n",
    "        Data_Generation_Ref = np.append(Data_Generation_Ref, T_ref_Vector_DataGen)\n",
    "Data_Generation_Ref = Data_Generation_Ref + CelsiusToKelvin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164e93c3",
   "metadata": {},
   "source": [
    "# Data Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22034637",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if NumberOfBuildings == 0:\n",
    "    raise ValueError(\"No building selected!\")\n",
    "    \n",
    "for CurrentBuilding in range(NumberOfBuildings):\n",
    "    \n",
    "    if CurrentBuilding == 0:\n",
    "        weather = \"CH_BS_Basel\"\n",
    "        env = energym.make(\"SimpleHouseRad-v0\", weather=weather, simulation_days=simulationDays)\n",
    "\n",
    "    elif CurrentBuilding == 1:\n",
    "        weather = \"CH_BS_Basel\"\n",
    "        env = energym.make(\"SimpleHouseRSla-v0\", weather=weather, simulation_days=simulationDays)\n",
    "\n",
    "    elif CurrentBuilding == 2:\n",
    "        weather = \"CH_BS_Basel\"\n",
    "        env = energym.make(\"SwissHouseRSlaW2W-v0\", weather=weather, simulation_days=simulationDays)\n",
    "\n",
    "    \n",
    "    if (simNewData == True):\n",
    "        # Load data and Energym model\n",
    "\n",
    "        # Setup simulation\n",
    "        steps = steps_per_Day*simulationDays*downsampleNumber\n",
    "        out_list = []\n",
    "        outputs = env.get_output()\n",
    "        controls = []\n",
    "        hour = 0\n",
    "        g = 0  \n",
    "        T_RefVec = np.array(T_RefVec)\n",
    "        T_RefVec = T_RefVec + CelsiusToKelvin\n",
    "\n",
    "        # Run simulation\n",
    "        control = {}\n",
    "        control['u'] = [(0.4)]\n",
    "        controls +=[ {p:control[p][0] for p in control} ]\n",
    "        outputs = env.step(control)\n",
    "        _,hour,_,_ = env.get_date()\n",
    "        out_list.append(outputs)\n",
    "        progress_bar = tqdm(total=steps-1, desc=\"Processing\")\n",
    "        for i in range((steps-1)):\n",
    "            progress_bar.update(1)\n",
    "            if (i%downsampleNumber==0):\n",
    "                Pcontrol = [(maximum((Data_Generation_Ref[g]-outputs['temRoo.T']) * P,0))]\n",
    "            control = {}\n",
    "            control['u'] = Pcontrol\n",
    "            controls +=[ {p:control[p][0] for p in control} ]\n",
    "            outputs = env.step(control)\n",
    "            _,hour,_,_ = env.get_date()\n",
    "            out_list.append(outputs)\n",
    "\n",
    "            if((i%8640)==2000):\n",
    "                g=g+1\n",
    "\n",
    "        progress_bar.close()\n",
    "        out_df_MPC = pd.DataFrame(out_list)\n",
    "        out_df_MPC = out_df_MPC.loc[:, ['temRoo.T', 'TOut.T', 'sunRad.y','heaPum.P']].rename(columns={'temRoo.T': 'Target_Temp', 'TOut.T': 'Ambient_Temp', 'sunRad.y': 'Solar Rad', 'heaPum.P': 'Energy'})\n",
    "        if CurrentBuilding == 0:\n",
    "            Building_1_data = out_df_MPC\n",
    "            print(Building_1_data)\n",
    "            Building_1_data = Building_1_data.iloc[1:]\n",
    "            Building_1_data.to_csv('Building_1_data.csv', index=False)\n",
    "            \n",
    "        elif CurrentBuilding == 1:\n",
    "            Building_2_data = out_df_MPC\n",
    "            print(Building_2_data)\n",
    "            Building_2_data = Building_2_data.iloc[1:]\n",
    "            Building_2_data.to_csv('Building_2_data.csv', index=False)\n",
    "            \n",
    "        elif CurrentBuilding == 2:\n",
    "            Building_3_data = out_df_MPC\n",
    "            print(Building_3_data)\n",
    "            Building_3_data = Building_3_data.iloc[1:]\n",
    "            Building_3_data.to_csv('Building_3_data.csv', index=False)\n",
    "            \n",
    "\n",
    "    elif (simNewData == False):\n",
    "        Building_1_data = pd.read_csv('Building_1_data.csv')\n",
    "        Building_2_data = pd.read_csv('Building_2_data.csv')\n",
    "        Building_3_data = pd.read_csv('Building_3_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075a3992",
   "metadata": {},
   "source": [
    "# Data handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91321117",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if (downsampleNumber != 1):    \n",
    "    Building_data_downsampled = extract_every_nth_row(Building_3_data, downsampleNumber)\n",
    "    Building_data_downsampled.reset_index(drop=True, inplace=True)\n",
    "else:\n",
    "    Building_data_downsampled = Building_3_data\n",
    "\n",
    "# Adding \"change in target temperature\" and \"Ambient temperature diff. with internal temp\" to the data:\n",
    "T_delta = Building_data_downsampled['Target_Temp'].diff().dropna()\n",
    "T_delta.reset_index(drop=True, inplace=True)\n",
    "Building_data_downsampled['Change_Target_Temp'] = T_delta\n",
    "u_delta = Building_data_downsampled['Energy'].diff().dropna()\n",
    "u_delta.reset_index(drop=True, inplace=True)\n",
    "u_delta **= 2 \n",
    "Building_data_downsampled['Change_Energy'] = u_delta\n",
    "Building_data_downsampled = Building_data_downsampled.iloc[:-1]\n",
    "Building_data_downsampled['Ambient_temp_diff']=Building_data_downsampled['Ambient_Temp']-Building_data_downsampled['Target_Temp']\n",
    "\n",
    "#Create shifted outputs for nRnC model to use.\n",
    "shifted_columns = []\n",
    "for i in range(4):\n",
    "    shifted_col = Building_data_downsampled['Change_Target_Temp'].shift(i + 1)\n",
    "    shifted_col.name = f'Change_Target_Temp_shifted_{i + 1}'\n",
    "    shifted_columns.append(shifted_col)\n",
    "\n",
    "# Concatenate the shifted columns with the original dataframe\n",
    "Building_data_downsampled = pd.concat([Building_data_downsampled] + shifted_columns, axis=1)\n",
    "Building_data_downsampled = Building_data_downsampled.dropna()\n",
    "Building_data_downsampled.reset_index(drop=True, inplace=True)\n",
    "\n",
    "#Normalizing the data:\n",
    "\n",
    "data_mean = Building_data_downsampled.mean()\n",
    "data_std = Building_data_downsampled.std()\n",
    "\n",
    "data_std['Change_Target_Temp_shifted_1'] = data_std['Change_Target_Temp']\n",
    "data_std['Change_Target_Temp_shifted_2'] = data_std['Change_Target_Temp']\n",
    "data_std['Change_Target_Temp_shifted_3'] = data_std['Change_Target_Temp']\n",
    "data_std['Change_Target_Temp_shifted_4'] = data_std['Change_Target_Temp']\n",
    "data_mean['Change_Target_Temp_shifted_1'] = data_mean['Change_Target_Temp']\n",
    "data_mean['Change_Target_Temp_shifted_2'] = data_mean['Change_Target_Temp']\n",
    "data_mean['Change_Target_Temp_shifted_3'] = data_mean['Change_Target_Temp']\n",
    "data_mean['Change_Target_Temp_shifted_4'] = data_mean['Change_Target_Temp']\n",
    "\n",
    "data_max = abs(data_mean) + 3*data_std\n",
    "\n",
    "non_zero_diff = abs(Building_data_downsampled['Energy'].diff().dropna()[Building_data_downsampled['Energy'].diff().dropna() != 0])\n",
    "max_u_delta = (non_zero_diff**2).mean() + 3 * (non_zero_diff**2).std()\n",
    "max_u_delta_non_square = non_zero_diff.mean() + 3 * non_zero_diff.std()\n",
    "max_y = data_max['Change_Target_Temp']\n",
    "max_x = data_max['Target_Temp']\n",
    "max_u = data_max['Energy']\n",
    "#max_u_delta = data_max['Change_Energy']\n",
    "max_d = data_max[['Ambient_Temp','Solar Rad', 'Ambient_temp_diff']].to_numpy()\n",
    "std_y = data_std['Change_Target_Temp']\n",
    "std_x = data_std['Target_Temp']\n",
    "std_u = data_std['Energy']\n",
    "std_u_delta = data_std['Change_Energy']\n",
    "std_d = data_std[['Ambient_Temp','Solar Rad', 'Ambient_temp_diff']].to_numpy()\n",
    "\n",
    "#Building_data_downsampled_scaled = normalize(Building_data_downsampled, data_mean, data_std)\n",
    "Building_data_downsampled_scaled = normalize(Building_data_downsampled, data_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d39b06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_max=Building_data_downsampled.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d1610c",
   "metadata": {},
   "source": [
    "# \"Yt\"- box model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebcaefdd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#%run Functions.ipynb\n",
    "# Define the number of models\n",
    "num_models = 1 + len(prevOutputRArray)\n",
    "\n",
    "# Arrays for minimum loss values and resistance values matrix\n",
    "min_loss_values = []\n",
    "resistance_values_matrix = np.zeros((num_models, (3+len(prevOutputRArray))))\n",
    "\n",
    "if trainNewModel:\n",
    "    for i in range(num_models):\n",
    "        print(f\"Training model: {i}\")\n",
    "        minCost, R_Ambient_Temp, R_Heat_Pump, R_Solar_Rad, prevOutputRArray = R1C1_Model_Train_flex(Building_data_downsampled_scaled, dt, trainPredHours, chunkTrainingPercentage, Plot_Modelperformance, prevOutputRArray, i)\n",
    "        \n",
    "        # Append the minimum loss value to the list\n",
    "        min_loss_values.append(minCost)\n",
    "        \n",
    "        # Save the resistance values in the matrix\n",
    "        resistance_values_matrix[i, :] = [R_Ambient_Temp, R_Heat_Pump, R_Solar_Rad] + list(prevOutputRArray)\n",
    "    \n",
    "    # Save the arrays\n",
    "    np.save('min_loss_values.npy', min_loss_values)\n",
    "    np.save('resistance_values_matrix.npy', resistance_values_matrix)\n",
    "    \n",
    "    # Load the minimum loss values\n",
    "    min_loss_values = np.load('min_loss_values.npy')\n",
    "\n",
    "    # Find the best observed loss\n",
    "    best_loss = min(min_loss_values)\n",
    "\n",
    "    # Define the threshold (105% higher than the best observed loss)\n",
    "    threshold = best_loss * 1.10\n",
    "\n",
    "    # Iterate through the models to find the lowest complexity model within the threshold\n",
    "    chosen_model = None\n",
    "    for i, loss in enumerate(min_loss_values):\n",
    "        if loss <= threshold:\n",
    "            chosen_model = i\n",
    "            break\n",
    "\n",
    "    # Print the chosen model\n",
    "    if chosen_model is not None:\n",
    "        print(f\"The chosen model is Model {chosen_model} with a minimum loss of {min_loss_values[chosen_model]}.\")\n",
    "        with open('variables.txt', 'w') as file:\n",
    "            file.write(f\"{chosen_model}\\n{resistance_values_matrix[chosen_model][0]}\\n{resistance_values_matrix[chosen_model][1]}\\n{resistance_values_matrix[chosen_model][2]}\\n{resistance_values_matrix[chosen_model][3]}\\n{resistance_values_matrix[chosen_model][4]}\\n{resistance_values_matrix[chosen_model][5]}\\n{resistance_values_matrix[chosen_model][6]}\")\n",
    "\n",
    "        R_Ambient_Temp = resistance_values_matrix[chosen_model,0]\n",
    "        R_Heat_Pump = resistance_values_matrix[chosen_model,1]\n",
    "        R_Solar_Rad = resistance_values_matrix[chosen_model,2]\n",
    "        prevOutputRArray = resistance_values_matrix[chosen_model,3:]\n",
    "    else:\n",
    "        \n",
    "        print(\"No model meets the threshold.\")\n",
    "        \n",
    "else:\n",
    "    print(\"Accessing previous model\")\n",
    "    with open('variables.txt', 'r') as file:\n",
    "        chosen_model = np.round(float(file.readline().strip())).astype(int)\n",
    "        R_Ambient_Temp = float(file.readline().strip())\n",
    "        R_Heat_Pump = float(file.readline().strip())\n",
    "        R_Solar_Rad = float(file.readline().strip())\n",
    "        \n",
    "        prevOutputRArray = []  # Initialize an empty list to store the lists of floating-point numbers\n",
    "        for _ in range(chosen_model):\n",
    "            line = float(file.readline().strip())\n",
    "            prevOutputRArray.append(line)\n",
    "            \n",
    "    print(f\"Chosen model order is: {chosen_model}\")\n",
    "    print(f\"Estimated R_amb: {R_Ambient_Temp}\")\n",
    "    print(f\"Estimated R_q: {R_Heat_Pump}\")\n",
    "    print(f\"Estimated R_sun: {R_Solar_Rad}\")\n",
    "    if chosen_model >= 1:\n",
    "        print(f\"Estimated R_1: {prevOutputRArray[0]}\")\n",
    "    if chosen_model >= 2:\n",
    "        print(f\"Estimated R_2: {prevOutputRArray[1]}\")\n",
    "    if chosen_model >= 3:\n",
    "        print(f\"Estimated R_3: {prevOutputRArray[2]}\")\n",
    "    if chosen_model >= 4:\n",
    "        print(f\"Estimated R_4: {prevOutputRArray[3]}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518ecbac",
   "metadata": {},
   "source": [
    "# nRnC model outputs for generated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291f3c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Choosing points per axis. Magic number 3 is chosen, since the baseline nRnC model has 3 elements. \n",
    "N = np.round(nth_root(nRnCOutputSamples, 3+chosen_model)).astype(int)  # N points along (3+chosen_model) axis = 64.000.000 samples.'.\n",
    "\n",
    "# Create an empty dictionary to store vectors for each feature\n",
    "feature_vectors = {}\n",
    "\n",
    "# Iterate through each feature\n",
    "for feature in Building_data_downsampled_scaled.columns:\n",
    "    # Generate evenly spaced values\n",
    "    values = normalize(np.linspace(-5 * data_std[feature] + data_mean[feature], 5 * data_std[feature] + data_mean[feature], N), data_max[feature])\n",
    "     \n",
    "    # Store the vector for the feature\n",
    "    feature_vectors[feature] = values\n",
    "    \n",
    "# Determine which shifted vectors to keep based on chosen_model value\n",
    "if chosen_model == 0:\n",
    "    shifted_vectors_to_keep = []\n",
    "else:\n",
    "    shifted_vectors_to_keep = ['Change_Target_Temp_shifted_{}'.format(i) for i in range(1, chosen_model + 1)]\n",
    "\n",
    "# Remove shifted vectors not to keep\n",
    "features_to_check = list(feature_vectors.keys())[-4:]  # Select last four features\n",
    "for feature in features_to_check:\n",
    "    if feature not in shifted_vectors_to_keep:\n",
    "        feature_vectors.pop(feature, None)\n",
    "        \n",
    "feature_vectors.pop('Change_Target_Temp', None)\n",
    "feature_vectors.pop('Target_Temp', None)\n",
    "feature_vectors.pop('Ambient_Temp', None) \n",
    "feature_vectors.pop('Change_Energy', None)\n",
    "\n",
    "# Create a list of feature names and their corresponding vectors\n",
    "feature_names = list(feature_vectors.keys())\n",
    "feature_values = list(feature_vectors.values())\n",
    "\n",
    "# Generate all possible combinations of feature values\n",
    "combinations = product(*feature_values)\n",
    "\n",
    "# Create a list of dictionaries where each dictionary represents a row in the dataset\n",
    "physics_data = []\n",
    "for combo in combinations:\n",
    "    row_dict = {feature_names[i]: combo[i] for i in range(len(feature_names))}\n",
    "    physics_data.append(row_dict)\n",
    "\n",
    "#Generate vector of means and std. deviations for the three features used in Physics-Loss data.\n",
    "#hysics_mean = data_mean[['Solar Rad','Energy','Ambient_temp_diff']]\n",
    "#physics_std = data_std[['Solar Rad','Energy','Ambient_temp_diff']]\n",
    "\n",
    "# Create a DataFrame from the list of dictionaries\n",
    "physics_data_panda_scaled = pd.DataFrame(physics_data)\n",
    "\n",
    "progress_bar = tqdm(total=len(physics_data_panda_scaled), desc=\"Generating Outputs\")\n",
    "\n",
    "nRnCPhysicsOutput = []\n",
    "for i in range(len(physics_data_panda_scaled)):\n",
    "    progress_bar.update(1)\n",
    "    nRnCPhysicsOutput.append(r1c1_model_flex(R_Ambient_Temp, R_Heat_Pump, R_Solar_Rad, physics_data_panda_scaled.iloc[i, 2], physics_data_panda_scaled.iloc[i, 1], physics_data_panda_scaled.iloc[i,0], physics_data_panda_scaled.iloc[i,3:].values, prevOutputRArray, chosen_model))\n",
    "\n",
    "progress_bar.close()    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38834500",
   "metadata": {},
   "source": [
    "# Batch Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b936e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your split ratio (e.g., 0.8 for 80% train, 20% test)\n",
    "split_ratio = 0.8\n",
    "batch_size_minutes = 60 * trainPredHours\n",
    "chunk_size = 1000  # Adjust the chunk size as needed\n",
    "\n",
    "nRnCPhysicsOutput_df = pd.DataFrame(nRnCPhysicsOutput)\n",
    "\n",
    "nRnCPhysics_df = pd.concat([physics_data_panda_scaled, nRnCPhysicsOutput_df], axis=1)\n",
    "\n",
    "# Define batch size based on time and cut off rest data\n",
    "batch_size_sim = math.ceil(batch_size_minutes / timeInterval)\n",
    "print(\"Batch Size: \", batch_size_sim)\n",
    "CutoffLength = len(Building_data_downsampled_scaled) % batch_size_sim\n",
    "if CutoffLength != 0:\n",
    "    Building_data = Building_data_downsampled_scaled.iloc[:-CutoffLength, :]\n",
    "    print(f\"Removing {CutoffLength} samples from sim data!\")\n",
    "else:\n",
    "    Building_data = Building_data_downsampled_scaled\n",
    "\n",
    "# Create batches for Building_data\n",
    "batches = create_batches(Building_data, batch_size_sim)\n",
    "np.random.shuffle(batches)\n",
    "\n",
    "# Concatenate batches back into DataFrame\n",
    "Building_data_shuffled = pd.concat(batches, ignore_index=True)\n",
    "\n",
    "# Split shuffled DataFrame into train and test based on the number of batches\n",
    "split_index = int(len(batches) * split_ratio)\n",
    "train_batches_sim, test_batches_sim = batches[:split_index], batches[split_index:]\n",
    "\n",
    "# Concatenate train and test batches back into DataFrames\n",
    "train_data_sim = pd.concat(train_batches_sim, ignore_index=True)\n",
    "test_data_sim = pd.concat(test_batches_sim, ignore_index=True)\n",
    "\n",
    "# Reset the index of train_data and test_data\n",
    "train_data_sim.reset_index(drop=True, inplace=True)\n",
    "test_data_sim.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Split nRnCPhysicsOutput into batches matching the number of batches for Building_data\n",
    "num_batches = len(batches)\n",
    "\n",
    "print(\"Number of batches: \", len(batches))\n",
    "print(\"Kasper se her\", len(nRnCPhysics_df))\n",
    "removeAmount = len(nRnCPhysics_df) % len(batches)\n",
    "if removeAmount != 0:\n",
    "    nRnCPhysics_df = nRnCPhysics_df.iloc[:-removeAmount, :]\n",
    "    print(f\"Removing {removeAmount} samples from nRnC data!\")\n",
    "\n",
    "batch_size_nRnC = np.round(len(nRnCPhysics_df)/num_batches).astype(int)\n",
    "nRnCPhysicsOutput_batches = np.array_split(nRnCPhysics_df, num_batches)\n",
    "\n",
    "# Shuffle the batches of nRnCPhysicsOutput\n",
    "np.random.shuffle(nRnCPhysicsOutput_batches)\n",
    "\n",
    "# Concatenate the shuffled batches back into a DataFrame\n",
    "shuffled_nRnCPhysicsOutput_df = pd.concat(nRnCPhysicsOutput_batches, ignore_index=True)\n",
    "\n",
    "# Split shuffled DataFrame into train and test based on the number of batches\n",
    "train_batches_nRnC, test_batches_nRnC = nRnCPhysicsOutput_batches[:split_index], nRnCPhysicsOutput_batches[split_index:]\n",
    "\n",
    "# Concatenate train and test batches back into DataFrames\n",
    "train_data_nRnC = pd.concat(train_batches_nRnC, ignore_index=True)\n",
    "test_data_nRnC = pd.concat(test_batches_nRnC, ignore_index=True)\n",
    "\n",
    "# Reset the index of train_data and test_data\n",
    "train_data_nRnC.reset_index(drop=True, inplace=True)\n",
    "test_data_nRnC.reset_index(drop=True, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01acc7e2",
   "metadata": {},
   "source": [
    "# Sorting to output and input data and trimming datasets to correct size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53688dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seperate output and input data in sim dataset\n",
    "output_data_sim = train_data_sim['Change_Target_Temp'].copy()\n",
    "input_data_sim = train_data_sim.copy()\n",
    "condition_data = train_data_sim[['Target_Temp','Ambient_Temp']].copy()\n",
    "columns_to_drop = ['Target_Temp','Ambient_Temp','Change_Target_Temp','Change_Energy'] +  [f'Change_Target_Temp_shifted_{i}' for i in range(chosen_model+1, 5)]\n",
    "input_data_sim = input_data_sim.drop(columns=columns_to_drop)\n",
    "\n",
    "output_data_sim_test = test_data_sim['Change_Target_Temp'].copy()\n",
    "input_data_sim_test = test_data_sim.copy()\n",
    "input_data_sim_test = input_data_sim_test.drop(columns=columns_to_drop)\n",
    "input_data_sim_tensor = torch.tensor(input_data_sim_test.to_numpy(), dtype=torch.float32)\n",
    "\n",
    "# Seperate output and input data in nRnC dataset\n",
    "train_data_nRnC.rename(columns={0: 'Change_Target_Temp'}, inplace=True)\n",
    "test_data_nRnC.rename(columns={0: 'Change_Target_Temp'}, inplace=True)\n",
    "\n",
    "output_data_nRnC = train_data_nRnC['Change_Target_Temp'].copy()\n",
    "input_data_nRnC = train_data_nRnC.copy()\n",
    "columns_to_drop2 = ['Change_Target_Temp']\n",
    "input_data_nRnC = input_data_nRnC.drop(columns=columns_to_drop2)\n",
    "\n",
    "output_data_nRnC_test = test_data_nRnC['Change_Target_Temp'].copy()\n",
    "input_data_nRnC_test = test_data_nRnC.copy()\n",
    "input_data_nRnC_test = input_data_nRnC_test.drop(columns=columns_to_drop2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f1506d",
   "metadata": {},
   "source": [
    "# Physic Informed Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90209283",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if trainNewModel:\n",
    "    layerValLosses = []\n",
    "    layerEpoch = 0\n",
    "    max_epochs = 50\n",
    "    best_layer_loss = 1000000000\n",
    "    min_layer_improvement = 1\n",
    "    \n",
    "    while True:\n",
    "        \n",
    "        # Initialize model\n",
    "        width = 2+5*(layerEpoch%2)+5*(layerEpoch//2)\n",
    "        depth = 1+layerEpoch//2\n",
    "        print(f\"Layer Epoch: {layerEpoch}, Layer width: {width}, Depth: {depth}\")\n",
    "        model = PINN(len(input_data_sim.columns), width, depth)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=2, verbose=True)\n",
    "        \n",
    "        # Initialize variables for early stopping\n",
    "        best_val_loss = 1000000000\n",
    "        epochs_no_improve = 0\n",
    "        max_epochs_stop = 6  # Maximum number of epochs with no improvement allowed\n",
    "        min_epoch_improve = 1 #in percent\n",
    "        # Training loop\n",
    "        for epoch in range(max_epochs):\n",
    "            \n",
    "            # Training phase\n",
    "            model.train()   # Puts the pytorch model into training mode\n",
    "            total_loss = 0.0\n",
    "            for batch_index in range(0, len(input_data_sim), batch_size_sim):\n",
    "                # Sim data loading\n",
    "                batch_input_data_sim = input_data_sim[batch_index:batch_index+batch_size_sim]\n",
    "                batch_input_data_sim_tensor = torch.tensor(batch_input_data_sim.values, dtype=torch.float32)\n",
    "                batch_output_data_sim = output_data_sim[batch_index:batch_index+batch_size_sim]\n",
    "                batch_output_data_sim_tensor = torch.tensor(batch_output_data_sim.values, dtype=torch.float32)\n",
    "                batch_condition_data = condition_data[batch_index:batch_index+batch_size_sim]\n",
    "                batch_condition_data_tensor = torch.tensor(batch_condition_data.values, dtype=torch.float32)\n",
    "\n",
    "                # Physics data loading\n",
    "                nRnRIndex = np.round((batch_index/batch_size_sim)*batch_size_nRnC).astype(int)\n",
    "                batch_input_data_nRnC = input_data_nRnC[nRnRIndex:nRnRIndex+batch_size_nRnC]\n",
    "                batch_input_data_nRnC_tensor = torch.tensor(batch_input_data_nRnC.values, dtype=torch.float32)\n",
    "                batch_output_data_nRnC = output_data_nRnC[nRnRIndex:nRnRIndex+batch_size_nRnC]\n",
    "                batch_output_data_nRnC_tensor = torch.tensor(batch_output_data_nRnC.values, dtype=torch.float32)\n",
    "\n",
    "                optimizer.zero_grad()    \n",
    "\n",
    "                data_loss = PINN_loss_with_BPTT(model, batch_input_data_sim_tensor, batch_output_data_sim_tensor, batch_condition_data_tensor)\n",
    "                physics_loss = PINN_loss_direct(model, batch_input_data_nRnC_tensor, batch_output_data_nRnC_tensor)\n",
    "                loss = data_loss + physics_loss\n",
    "\n",
    "                # Backpropagation and optimization\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                total_loss += loss.item()\n",
    "\n",
    "            # Print training loss for the epoch\n",
    "            train_loss = total_loss / num_batches\n",
    "            \n",
    "            \n",
    "            # Validation phase\n",
    "            model.eval()  # Puts the pytorch model into evaluation mode\n",
    "            with torch.no_grad():\n",
    "                predicted_output_test = model(input_data_sim_tensor)\n",
    "                val_loss = np.sum((output_data_sim_test - predicted_output_test.squeeze().detach().numpy())**2)\n",
    "                print(f\"Epoch {epoch}, Training Loss: {train_loss}, Validation Loss: {val_loss}\")\n",
    "                scheduler.step(val_loss)\n",
    "                # Early stopping check\n",
    "                if val_loss < (best_val_loss-(min_epoch_improve*best_val_loss)/100) or val_loss > best_val_loss:\n",
    "                    best_val_loss = val_loss\n",
    "                    epochs_no_improve = 0\n",
    "                else:\n",
    "                    epochs_no_improve += 1\n",
    "                    if epochs_no_improve == max_epochs_stop:\n",
    "                        best_val_loss = val_loss\n",
    "                        print(\"Early stopping triggered.\")\n",
    "                        break\n",
    "            \n",
    "        # Layer improvement check\n",
    "        layerValLosses.append(best_val_loss)\n",
    "        improvement = ((best_layer_loss - val_loss) / best_layer_loss) * 100\n",
    "        if improvement < min_layer_improvement:    #If we fail the test, break and discard the last model\n",
    "            print(f\"No significant improvement in layer loss. Stopping training.\")\n",
    "            chosenLayerSize = layerEpoch-1        #Choose previous model (might be better, or atleast simpler and just as good)\n",
    "            break\n",
    "        else:                                      #If we succeed, save the model and train one more\n",
    "            torch.save(model.state_dict(), 'model.pth')\n",
    "            \n",
    "        \n",
    "        # Increment epoch and best loss\n",
    "        layerEpoch += 1\n",
    "        best_layer_loss = best_val_loss\n",
    "        print(f\"Layer Epoch: {layerEpoch}, Best Validation Loss: {best_val_loss}\")\n",
    "\n",
    "\n",
    "        \n",
    "    # When finished, save layer size variable. Model is saved during training\n",
    "    with open('PINNvariables.txt', 'w') as file:\n",
    "        file.write(f\"{chosenLayerSize}\")\n",
    "else:\n",
    "    # Load the model\n",
    "    \n",
    "    print(\"Accessing previous model\")\n",
    "    with open('PINNvariables.txt', 'r') as file:\n",
    "        chosenLayerSize = np.round(float(file.readline().strip())).astype(int)\n",
    "        \n",
    "    model = PINN(len(input_data_sim.columns), 2+5*(chosenLayerSize%2)+5*(chosenLayerSize//2), 1+chosenLayerSize//2)\n",
    "    model.load_state_dict(torch.load('model.pth'))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bba6982",
   "metadata": {},
   "source": [
    "# Performance Comparison for chosen nRnC / PINN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6570922",
   "metadata": {},
   "outputs": [],
   "source": [
    "# actual data vs the models\n",
    "input_data_sim_tensor = torch.tensor(input_data_sim_test.to_numpy(), dtype=torch.float32)\n",
    "predicted_output_test = model(input_data_sim_tensor)\n",
    "\n",
    "nRnCSimOutput = []\n",
    "for i in range(len(input_data_sim_test)):\n",
    "    progress_bar.update(1)\n",
    "    nRnCSimOutput.append(r1c1_model_flex(R_Ambient_Temp, R_Heat_Pump, R_Solar_Rad, input_data_sim_test.iloc[i, 2], input_data_sim_test.iloc[i, 1], input_data_sim_test.iloc[i,0], input_data_sim_test.iloc[i,3:].values, prevOutputRArray, chosen_model))\n",
    "\n",
    "\n",
    "# Sample data for demonstration\n",
    "actual_data = output_data_sim_test\n",
    "predicted_data = pd.Series(predicted_output_test.squeeze().detach().numpy())\n",
    "\n",
    "# Create trace for actual data\n",
    "\n",
    "trace_actual = go.Scatter(\n",
    "    x=list(range(len(actual_data))),\n",
    "    y=actual_data,\n",
    "    mode='lines',\n",
    "    name='True output',\n",
    "    line=dict(color='blue')\n",
    ")\n",
    "\n",
    "# Create trace for predicted data\n",
    "trace_PINN_predicted = go.Scatter(\n",
    "    x=list(range(len(predicted_data))),\n",
    "    y=predicted_data,\n",
    "    mode='lines',\n",
    "    name='PINN prediction',\n",
    "    line=dict(color='red')\n",
    ")\n",
    "\n",
    "# Create trace for predicted data\n",
    "trace_nRnC_predicted = go.Scatter(\n",
    "    x=list(range(len(predicted_data))),\n",
    "    y=nRnCSimOutput,\n",
    "    mode='lines',\n",
    "    name='nRnC prediction',\n",
    "    line=dict(color='green')\n",
    ")\n",
    "\n",
    "# Create layout\n",
    "layout = go.Layout(\n",
    "    title='Comparison of nRnC and PINN models',\n",
    "    xaxis=dict(title='Sample'),\n",
    "    yaxis=dict(title='dTdt Value')\n",
    ")\n",
    "\n",
    "# Create figure\n",
    "fig = go.Figure(data=[trace_actual, trace_PINN_predicted, trace_nRnC_predicted], layout=layout)\n",
    "\n",
    "# Show plot\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b49479",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Test of predictive performance\n",
    "#nRnC pred test\n",
    "if Plot_Modelperformance == 1:\n",
    "    \n",
    "    deviationArrnRnC = np.zeros(len(predTestArray))\n",
    "    Q = Building_data_downsampled_scaled['Energy']\n",
    "    T_amb_diff = Building_data_downsampled_scaled['Ambient_temp_diff']\n",
    "    Sun = Building_data_downsampled_scaled['Solar Rad']\n",
    "    T_observed = Building_data_downsampled_scaled['Target_Temp']\n",
    "    T_amb = Building_data_downsampled_scaled['Ambient_Temp']\n",
    "    previous_dTdt_values = Building_data_downsampled_scaled[['Change_Target_Temp_shifted_1','Change_Target_Temp_shifted_2','Change_Target_Temp_shifted_3','Change_Target_Temp_shifted_4']]\n",
    "\n",
    "    for k in range(len(predTestArray)):\n",
    "        trainPredSteps = int(round((predTestArray[k] / 24) / dt))\n",
    "\n",
    "        R1C1_pred = np.zeros(len(Building_data_downsampled_scaled))\n",
    "        for i in range(len(Building_data_downsampled_scaled) - trainPredSteps):\n",
    "            temp_pred = T_observed[i]\n",
    "            T_amb_diff_temp = T_amb_diff[i]\n",
    "            previous_dTdt_values_temp = previous_dTdt_values.iloc[i].values\n",
    "            for j in range(trainPredSteps):\n",
    "                dTdt = r1c1_model_flex(R_Ambient_Temp, R_Heat_Pump, R_Solar_Rad, T_amb_diff_temp, Q[i + j], Sun[i + j], previous_dTdt_values_temp, prevOutputRArray, chosen_model)\n",
    "                temp_pred = normalize(denormalize(temp_pred, max_x) + denormalize(dTdt, max_y), max_x)\n",
    "                T_amb_diff_temp = normalize(denormalize(T_amb[i + j + 1], max_d[0]) - denormalize(temp_pred, max_x), max_d[2])\n",
    "                previous_dTdt_values_temp = np.insert(previous_dTdt_values_temp[:-1], 0, dTdt)\n",
    "            R1C1_pred[i + trainPredSteps] = temp_pred\n",
    "            \n",
    "        predict = np.array(denormalize(R1C1_pred, max_x)).reshape(1, -1)\n",
    "        data_output = np.array(denormalize(T_observed, max_x)).reshape(1, -1)\n",
    "        deviationArrnRnC[k] = np.mean(np.abs(predict[0, trainPredSteps:] - data_output[0, trainPredSteps:]))\n",
    "\n",
    "    #PINN pred test\n",
    "\n",
    "    deviationArrPINN = np.zeros(len(predTestArray))\n",
    "    Q = Building_data_downsampled_scaled['Energy']\n",
    "    T_amb_diff = Building_data_downsampled_scaled['Ambient_temp_diff']\n",
    "    Sun = Building_data_downsampled_scaled['Solar Rad']\n",
    "    T_observed = Building_data_downsampled_scaled['Target_Temp']\n",
    "    T_amb = Building_data_downsampled_scaled['Ambient_Temp']\n",
    "    previous_dTdt_values = Building_data_downsampled_scaled[['Change_Target_Temp_shifted_1','Change_Target_Temp_shifted_2','Change_Target_Temp_shifted_3','Change_Target_Temp_shifted_4']]\n",
    "\n",
    "    for k in range(len(predTestArray)):\n",
    "        trainPredSteps = int(round((predTestArray[k] / 24) / dt))\n",
    "\n",
    "        R1C1_pred = np.zeros(len(Building_data_downsampled_scaled))\n",
    "        for i in range(len(Building_data_downsampled_scaled) - trainPredSteps):\n",
    "            temp_pred = T_observed[i]\n",
    "            T_amb_diff_temp = T_amb_diff[i]\n",
    "            previous_dTdt_values_temp = previous_dTdt_values.iloc[i].values\n",
    "\n",
    "            for j in range(trainPredSteps):\n",
    "                input_values = []\n",
    "                input_values.append(Sun[i + j])\n",
    "                input_values.append(Q[i + j])\n",
    "                input_values.append(T_amb_diff_temp)\n",
    "                for value in previous_dTdt_values_temp[:chosen_model]:\n",
    "                    input_values.append(value) \n",
    "                    \n",
    "                input_values = torch.tensor(input_values, dtype=torch.float32, requires_grad=False)\n",
    "                dTdt = model(input_values)\n",
    "                temp_pred = normalize(denormalize(temp_pred, max_x) + denormalize(dTdt.detach().numpy(), max_y), max_x)\n",
    "                \n",
    "                T_amb_diff_temp = normalize(denormalize(T_amb[i + j + 1], max_d[0]) - denormalize(temp_pred, max_x), max_d[2])\n",
    "                T_amb_diff_temp = T_amb_diff_temp[0]\n",
    "                previous_dTdt_values_temp = np.insert(previous_dTdt_values_temp[:-1], 0, dTdt.detach().numpy())\n",
    "            R1C1_pred[i + trainPredSteps] = temp_pred\n",
    "            \n",
    "        predict = np.array(denormalize(R1C1_pred, max_x)).reshape(1, -1)\n",
    "        data_output = np.array(denormalize(T_observed, max_x)).reshape(1, -1)\n",
    "        deviationArrPINN[k] = np.mean(np.abs(predict[0, trainPredSteps:] - data_output[0, trainPredSteps:]))\n",
    "\n",
    "    #Plotting   \n",
    "\n",
    "    fig = px.line(x=predTestArray, y=[deviationArrnRnC, deviationArrPINN], \n",
    "                  labels={'x': 'Hours predicted', 'y': 'Mean Squared Error', 'y0': 'nRnC', 'y1': 'PINN'}, \n",
    "                  title='Prediction performance')\n",
    "    # Set the text describing the values on the y-axis\n",
    "    fig.update_layout(yaxis=dict(title='Mean Error'), legend_title='Data from')\n",
    "    fig.update_traces(name='nRnC', selector=dict(name='wide_variable_0'))\n",
    "    fig.update_traces(name='PINN', selector=dict(name='wide_variable_1'))\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74afc31f",
   "metadata": {},
   "source": [
    "# MPC Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a113b4e0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#simulate forecast\n",
    "%run Functions.ipynb\n",
    "forecast = env.get_forecast(forecast_length=(steps_per_Day*(final_sim_days+1)*downsampleNumber))\n",
    "columns_to_extract = ['TOut.T', 'sunRad.y']\n",
    "column_values = [forecast[column] for column in columns_to_extract]\n",
    "forecastData = np.array(column_values).T\n",
    "#collect data\n",
    "T_amb = forecastData[:,0]\n",
    "\n",
    "Rad = forecastData[:,1]\n",
    "#normalize data\n",
    "Solar_Rad_Forecast = normalize(Rad, max_d[1])\n",
    "Ambient_Temp_Forecast = T_amb\n",
    "\n",
    "# Reset simulation steps\n",
    "if NumberOfBuildings == 1:\n",
    "    weather = \"CH_BS_Basel\"\n",
    "    env = energym.make(\"SimpleHouseRad-v0\", weather=weather, simulation_days=final_sim_days)\n",
    "    print('SimpleHouseRad')\n",
    "elif NumberOfBuildings == 2:\n",
    "    weather = \"CH_BS_Basel\"\n",
    "    env = energym.make(\"SimpleHouseRSla-v0\", weather=weather, simulation_days=final_sim_days)\n",
    "    print('SimpleHouseRSla')\n",
    "elif NumberOfBuildings == 3:\n",
    "    weather = \"CH_BS_Basel\"\n",
    "    env = energym.make(\"SwissHouseRSlaW2W-v0\", weather=weather, simulation_days=final_sim_days)\n",
    "    print('SwissHouseRSlaW2W')\n",
    "# Target_Temp_tensor = torch.tensor(Building_1_data['Target_Temp'].iloc[0]).reshape(1)\n",
    "# Ambient_temp_diff_tensor = torch.tensor(Building_1_data['Ambient_temp_diff'].iloc[0]).reshape(1)\n",
    "# Change_Target_Temp_tensor = torch.tensor(Building_1_data['Change_Target_Temp'].iloc[0]).reshape(1)\n",
    "# Change_Target_Temp_shifted_vector = torch.zeros(1, 4)\n",
    "\n",
    "weather = \"CH_BS_Basel\"\n",
    "env = energym.make(\"SwissHouseRSlaW2W-v0\", weather=weather, simulation_days=final_sim_days)\n",
    "print('SwissHouseRSlaW2W')\n",
    "\n",
    "Target_Temp = Building_1_data['Target_Temp'].iloc[0]\n",
    "Change_Target_Temp_shifted_vector = np.zeros(4)\n",
    "\n",
    "temp_input = 0 #initial condition of u\n",
    "\n",
    "\n",
    "#MOBO inits\n",
    "initial_u = -R_Heat_Pump * ((train_data_sim['Ambient_temp_diff'].mean()/R_Ambient_Temp) + (train_data_sim['Solar Rad'].mean()/R_Solar_Rad))\n",
    "param_space = [Real(0.0, 2000.0), Real(0.0, 2000.0)]\n",
    "initialIters = 3  \n",
    "initialHyperParamArray = [(1800,1800), (0,0), (900,900)] # This is spaghetti\n",
    "\n",
    "print(initialHyperParamArray)\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning, message=\"Values in x were outside bounds during a minimize step, clipping to bounds\")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\"X does not have valid feature names, but StandardScaler was fitted with feature names\")\n",
    "warnings.filterwarnings(\"ignore\", category=np.VisibleDeprecationWarning)\n",
    "\n",
    "progress_bar = tqdm(total=MPC_Iter-1, desc=\"Simulating\")\n",
    "\n",
    "out_list = []\n",
    "outputs = env.get_output()\n",
    "controls = []\n",
    "control = {}\n",
    "hour = 0\n",
    "observations = []\n",
    "if RunMPC == True:\n",
    "    for j in range((steps_per_Day*downsampleNumber)):    \n",
    "        progress_bar.update(1)\n",
    "\n",
    "        #Find input\n",
    "        if (j%downsampleNumber == 0):\n",
    "\n",
    "\n",
    "            if ChosenPredModel == 0:\n",
    "                res = minimize(PINN_MPC_Cost, u0, args=(\n",
    "                                Target_Temp, Change_Target_Temp_shifted_vector, Ambient_Temp_Forecast, \n",
    "                                Solar_Rad_Forecast, T_ref_Vector, T_ref_Lower, \n",
    "                                R, CFirstDay, C_deltaFirstDay, H_p, j, temp_input\n",
    "                               ), bounds = u_max, method=\"Nelder-Mead\")\n",
    "                temp_input = res.x[0]/1000\n",
    "\n",
    "            else:\n",
    "                res = minimize(nRnC_MPC_Cost, u0, args=(\n",
    "                                Target_Temp, Change_Target_Temp_shifted_vector, Ambient_Temp_Forecast, \n",
    "                                Solar_Rad_Forecast, T_ref_Vector, T_ref_Lower, \n",
    "                                R, CFirstDay, C_deltaFirstDay, H_p, j, temp_input, chosen_model\n",
    "                               ), bounds = u_max, method=\"SLSQP\")\n",
    "                temp_input = res.x[0]/1000\n",
    "\n",
    "        #take step in simulation with the computed control input       \n",
    "\n",
    "        control['u'] = [temp_input]\n",
    "        controls +=[ {p:control[p][0] for p in control} ]\n",
    "        outputs = env.step(control)\n",
    "        _,hour,_,_ = env.get_date()\n",
    "        out_list.append(outputs)\n",
    "        Target_Temp = out_list[-1]['temRoo.T'] \n",
    "\n",
    "        if j % downsampleNumber == 0 and j >= downsampleNumber:\n",
    "            Change_Target_Temp_shifted_vector = np.insert(Change_Target_Temp_shifted_vector[:-1], 0, normalize(Target_Temp - out_list[-4]['temRoo.T'], max_y))\n",
    "\n",
    "    if hyperParamTuner==0:\n",
    "\n",
    "        for j in range(initialIters):\n",
    "            MPC(initialHyperParamArray[j])\n",
    "\n",
    "        #acq_func = \"gp_hedge\"  # Adjust as needed\n",
    "        acq_func = \"LCB\"\n",
    "        # Define a custom kernel\n",
    "        kernel = \"RBF\"\n",
    "\n",
    "        # Create a Gaussian Process Regressor with the custom kernel\n",
    "        gp = GaussianProcessRegressor(kernel=kernel)\n",
    "\n",
    "        acq_optimizer = \"lbfgs\"  # Adjust as needed\n",
    "\n",
    "        result = gp_minimize(MPC,\n",
    "                             param_space,\n",
    "                             acq_func=acq_func,\n",
    "                             acq_optimizer=acq_optimizer,\n",
    "                             kappa=1,\n",
    "                             n_initial_points=3,\n",
    "                             n_calls=(int(final_sim_days - ((initialIters*paramTuningInterval)+1))//paramTuningInterval),\n",
    "                             x0=[obs[0] for obs in observations],\n",
    "                             y0=[obs[1] for obs in observations])\n",
    "\n",
    "    else:\n",
    "\n",
    "        # Set up the TPEMultiObjectiveSampler\n",
    "        sampler = MOTPESampler(seed=1)\n",
    "\n",
    "        # Define the study to optimize\n",
    "        study = optuna.create_study(sampler=sampler, directions=[\"minimize\", \"minimize\", \"minimize\", \"minimize\", \"minimize\"])  # Define 'minimize' for each objective\n",
    "\n",
    "        # Run optimization\n",
    "        study.optimize(MPC, n_trials=int(final_sim_days-1)//paramTuningInterval)  # Adjust n_trials as needed\n",
    "\n",
    "        # Access the Pareto front\n",
    "        pareto_front = study.trials_dataframe().filter(regex=(\".*_values$\")).values\n",
    "\n",
    "    progress_bar.close()\n",
    "    out_df_MPC = pd.DataFrame(out_list)\n",
    "    C_C_delta = np.array([item[0] for item in observations])\n",
    "    out_df_KPI = pd.DataFrame(C_C_delta, columns=['C', 'C_delta'])\n",
    "    KPIs = np.array([item[1] for item in observations])\n",
    "    if hyperParamTuner==0:\n",
    "        KPI_temp = pd.DataFrame(KPIs, columns=['KPI'])\n",
    "    else:\n",
    "        KPI_temp = pd.DataFrame(KPIs, columns=['Energy_delta', 'Ref_rise_penalty', 'Above_range'])\n",
    "        #KPI_temp = pd.DataFrame(KPIs, columns=['PIR', 'Energy', 'Energy_delta', 'Ref_rise_penalty', 'Above_range'])\n",
    "    out_df_KPI = pd.concat([out_df_KPI, KPI_temp], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c76656",
   "metadata": {},
   "source": [
    "# Plotting MPC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2973e405",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%run Functions.ipynb\n",
    "if RunMPC == True:\n",
    "    Plotting(out_df_MPC, T_ref_Lower, T_ref_Vector, final_sim_days)\n",
    "    KPIplotting(out_df_KPI)\n",
    "    if ChosenPredModel == 0:\n",
    "        if hyperParamTuner == 0:\n",
    "            out_df_MPC.to_csv('SOBO_PINN.csv', index=False)\n",
    "            out_df_KPI.to_csv('SOBO_PINN_KPI.csv', index=False)\n",
    "        if hyperParamTuner == 1:\n",
    "            out_df_MPC.to_csv('MOBO_PINN.csv', index=False)\n",
    "            out_df_KPI.to_csv('MOBO_PINN_KPI.csv', index=False)\n",
    "\n",
    "    if ChosenPredModel == 1: \n",
    "        if hyperParamTuner == 0:\n",
    "            out_df_MPC.to_csv('SOBO_nRnC.csv', index=False)\n",
    "            out_df_KPI.to_csv('SOBO_nRnC_KPI.csv', index=False)\n",
    "        if hyperParamTuner == 1:\n",
    "            out_df_MPC.to_csv('MOBO_nRnC.csv', index=False)\n",
    "            out_df_KPI.to_csv('MOBO_nRnC_KPI.csv', index=False)\n",
    "            \n",
    "elif RunMPC == False:\n",
    "#     SOBO_PINN_KPI = pd.read_csv(\"SOBO-PINN-KPI.csv\")\n",
    "#     MOBO_PINN_KPI = pd.read_csv(\"MOBO-PINN-KPI.csv\")\n",
    "    SOBO_nRnC_KPI = pd.read_csv(\"SOBO_nRnC_KPI.csv\")\n",
    "    MOBO_nRnC_KPI = pd.read_csv(\"MOBO_nRnC_KPI.csv\")\n",
    "    \n",
    "#     SOBO_PINN = pd.read_csv('SOBO-PINN.csv')\n",
    "#     MOBO_PINN = pd.read_csv('MOBO-PINN.csv')\n",
    "    SOBO_nRnC = pd.read_csv('SOBO_nRnC.csv')\n",
    "    MOBO_nRnC = pd.read_csv('MOBO_nRnC.csv')\n",
    "\n",
    "#     Plotting(SOBO_PINN, T_ref_Lower, T_ref_Vector, final_sim_days, \"SOBO_PINN\")\n",
    "#     KPIplotting(SOBO_PINN_KPI)\n",
    "#     Plotting(MOBO_PINN, T_ref_Lower, T_ref_Vector, final_sim_days, \"MOBO_PINN\")\n",
    "#     KPIplotting(MOBO_PINN_KPI)\n",
    "    Plotting(SOBO_nRnC, T_ref_Lower, T_ref_Vector, final_sim_days, \"SOBO_nRnC\")\n",
    "    KPIplotting(SOBO_nRnC_KPI)\n",
    "    Plotting(MOBO_nRnC, T_ref_Lower, T_ref_Vector, final_sim_days, \"MOBO_nRnC\")\n",
    "    KPIplotting(MOBO_nRnC_KPI)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2fc8fe4",
   "metadata": {},
   "source": [
    "# Computing KPIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c7b376",
   "metadata": {},
   "outputs": [],
   "source": [
    "Initialdays = 2\n",
    "out_df_MPC_KPI_New_nRnC = out_df_MPC.iloc[Initialdays * steps_per_Day * downsampleNumber:]\n",
    "\n",
    "time_in_range = 0\n",
    "total_time = len(out_df_MPC_KPI_New_nRnC)\n",
    "TIR_Lower = np.tile(T_ref_Lower, final_sim_days-1-Initialdays)\n",
    "TIR_Upper = np.tile(T_ref_Upper, final_sim_days-1-Initialdays)\n",
    "TIR_Lower = (TIR_Lower - CelsiusToKelvin).reshape(-1,1)\n",
    "TIR_Upper = (TIR_Upper - CelsiusToKelvin).reshape(-1,1)\n",
    "out_df_MPC_RoomTemp = np.array([out_df_MPC_KPI_New_nRnC['temRoo.T']-CelsiusToKelvin]).reshape(-1,1)\n",
    "out_df_MPC_Energy = np.array([out_df_MPC_KPI_New_nRnC['heaPum.P']]).reshape(-1,1)\n",
    "in_range_TIR = (out_df_MPC_RoomTemp >= TIR_Lower) & (out_df_MPC_RoomTemp <= TIR_Upper)\n",
    "in_range_TBR = (out_df_MPC_RoomTemp <= TIR_Lower)\n",
    "time_in_range = np.sum(in_range_TIR)\n",
    "time_below_range = np.sum(in_range_TBR)\n",
    "EnergyConsumption = np.sum(out_df_MPC_KPI_New_nRnC['heaPum.P'])\n",
    "EnergyDiffSquared = np.sum(np.diff(out_df_MPC_Energy.reshape(1,-1))**2)\n",
    "EnergyDiff = np.sum(abs(np.diff(out_df_MPC_Energy.reshape(1,-1))))\n",
    "\n",
    "percentage_below_range = (time_below_range / total_time) *100\n",
    "percentage_in_range = (time_in_range / total_time) * 100\n",
    "print(f\"Percentage in range: {percentage_in_range:.2f}%\")\n",
    "print(f\"Percentage below range: {percentage_below_range:.2f}%\")\n",
    "print(f\"Total Energy Consumption: {EnergyConsumption:.2e}\")\n",
    "print(f\"Energy Diff: {EnergyDiff:.2e}\")\n",
    "print(f\"Energy Diff Squared: {EnergyDiffSquared:.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59621058",
   "metadata": {},
   "source": [
    "# Plotting MPC simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff2d2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Plotting(out_df_MPC, T_ref_Lower, T_ref_Vector, final_sim_days, 'Name of plot')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
